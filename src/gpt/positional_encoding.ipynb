{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/designing-positional-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  3290, 26172,  1194,  3290]])\n",
      "50257\n",
      "tensor([ 0.0248,  0.0074, -0.0206, -0.0999, -0.0039, -0.0763, -0.1299, -0.1456,\n",
      "        -0.0880,  0.0881,  0.0426,  0.0459, -0.0709,  0.0687,  0.0449,  0.0836,\n",
      "        -0.1523,  0.0542,  0.0756, -0.0649, -0.0209, -0.0451, -0.0571,  0.1215,\n",
      "        -0.0255, -0.0319, -0.0295, -0.0034,  0.0197, -0.1797, -0.1374,  0.0840],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0248,  0.0074, -0.0206, -0.0999, -0.0039, -0.0763, -0.1299, -0.1456,\n",
      "        -0.0880,  0.0881,  0.0426,  0.0459, -0.0709,  0.0687,  0.0449,  0.0836,\n",
      "        -0.1523,  0.0542,  0.0756, -0.0649, -0.0209, -0.0451, -0.0571,  0.1215,\n",
      "        -0.0255, -0.0319, -0.0295, -0.0034,  0.0197, -0.1797, -0.1374,  0.0840],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "text = \"The dog chased another dog\"\n",
    "tokens = tok(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(tokens)\n",
    "\n",
    "print(tok.vocab_size)\n",
    "embedding = nn.Embedding(tok.vocab_size, 32)\n",
    "\n",
    "emb = embedding(tokens)\n",
    "qkv_linear = nn.Linear(32, 32*3, bias=False)\n",
    "q, k, v = torch.tensor_split(qkv_linear(emb), 3, dim=-1)\n",
    "mha = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "out = mha(q, k, v, need_weights=False)[0]\n",
    "\n",
    "dog1 = out[0, 1]\n",
    "dog2 = out[0, 4]\n",
    "print(dog1)\n",
    "print(dog2)\n",
    "print(torch.allclose(dog1, dog2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties:\n",
    "1. Unique encoding for position, regardless of sequence length. Same encoding if sequence of size 10 vs 100.\n",
    "2. Linear relation b/t encoded positions. Encodings for p, p+k should be linear (number line 2 to 5 is 3).\n",
    "3. Generalizes to longer sequences (longer sequences at inference than training)\n",
    "4. Deterministic, learnable process\n",
    "5. Extensible to multiple dimensions (images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer encoding:\n",
    "* 1, 2, 3, ...\n",
    "* very low snr b/c of scale compared to 0-clustered embedding values\n",
    "\n",
    "# Binary position encoding:\n",
    "* convert int position to binary, add to token embedding\n",
    "* periodic: lsb cycles b/t 0 and 1 for each token, msb cycles at a much slower rate\n",
    "* cons: jumpy discrete function\n",
    "\n",
    "# Sinusoidal position encoding:\n",
    "* $PE(\\text{pos}, 2i) = \\sin(\\frac{\\text{pos}}{10000^{2i/d}})$\n",
    "* $PE(\\text{pos}, 2i+1) = \\cos(\\frac{\\text{pos}}{10000^{2i/d}})$\n",
    "* evens are sin, odds are cos\n",
    "* pos is the token position index, i is the ith component in the positional encoding vecotr, d is model dim, 10000 is $\\theta$, base wavelength\n",
    "* for 1 pos: increasing wavelength (divided by larger denom, period increases, freq decreases)\n",
    "* relative position between positions is a rotation (linear transform by rotation matrix)\n",
    "\n",
    "\n",
    "Cons:\n",
    "* generate separate positional embedding vector and adds to token embedding\n",
    "* relative position is encoded as rotation, but we still store abs position (not really necessary) by adding PE to TE\n",
    "* $QK^T$ calculates affinities through dot product $a \\cdot b = |a| |b| cos(\\theta)$: rotate vectors instead of changing norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotary Positional Encoding:\n",
    "* rotates each 2d-pair in q and k\n",
    "* same higher freq rotation -> lower freq rotation\n",
    "* relative because positional encoding rotates vectors\n",
    "* if same position -> same rotation -> no change in dot product\n",
    "\n",
    "\n",
    "$ R(q, p) = \n",
    "\\begin{pmatrix}\n",
    "M_1 &    &        &     \\\\\n",
    "    & M_2 &       &     \\\\\n",
    "    &    & \\ddots &     \\\\\n",
    "    &    &        & M_{d/2} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}q_1 \\\\ q_2 \\\\ \\vdots \\\\ q_d \\end{pmatrix}\n",
    "$\n",
    "\n",
    "$ M_i = \n",
    "\\begin{pmatrix}\n",
    "cos(w_i p) & -sin(w_i p) \\\\\n",
    "sin(w_i p) & cos(w_i p) \\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "$w_i = \\frac{1}{10000^{2i/d}}$\n",
    "\n",
    "$ R(q, p) = \n",
    "\\begin{pmatrix} q_1 \\\\ q_2 \\\\ q3 \\\\ q4 \\\\ \\vdots \\\\ q_{d-1} \\\\ q_d \\end{pmatrix}\n",
    "\\begin{pmatrix} cos(p \\theta_1) \\\\ cos(p \\theta_1) \\\\ cos(p \\theta_2) \\\\ cos(p \\theta_2) \\\\ \\vdots \\\\ cos(p \\theta_{d/2}) \\\\ cos(p \\theta_{d/2}) \\end{pmatrix}\n",
    "+ \n",
    "\\begin{pmatrix} -q_2 \\\\ q_1 \\\\ -q4 \\\\ q3 \\\\ \\vdots \\\\ -q_d \\\\ q_{d-1} \\end{pmatrix}\n",
    "\\begin{pmatrix} sin(p \\theta_1) \\\\ sin(p \\theta_1) \\\\ sin(p \\theta_2) \\\\ sin(p \\theta_2) \\\\ \\vdots \\\\ sin(p \\theta_{d/2}) \\\\ sin(p \\theta_{d/2}) \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoFormer: https://arxiv.org/pdf/2104.09864  \n",
    "Eleuther ai post: https://blog.eleuther.ai/rotary-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy-tutorial-p8SQ0mzG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
